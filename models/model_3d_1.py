# -*- coding: utf-8 -*-
"""model_3D_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E0HrO5iYMcmJVvTkINcsUQgkwi_QVtNy
"""

import re
from collections import defaultdict
from tqdm import tqdm
import numpy as np
from PIL import Image
import os
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models

import numpy as np
import scipy.ndimage
import matplotlib.pyplot as plt
import cv2
from pathlib import Path
import os
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \
    confusion_matrix
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, InputLayer, Conv2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization, Activation, ReLU, Flatten, Dense, Add,\
    Dropout,MaxPooling2D,Concatenate,Reshape
from tensorflow.keras.models import Sequential, Model
import tensorflow as tf
import warnings
from pathlib import Path
# Tüm uyarıları kapat
warnings.filterwarnings("ignore")
import seaborn as sns

IMAGE_SIZE = 102
target_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE)

def rescale_to_shape(array, target_shape):
    factors = [t / s for s, t in zip(array.shape, target_shape)]
    return scipy.ndimage.zoom(array, zoom=factors, mode='nearest')

path = "/content/extracted_files/data_npy"
DATA_PATH = Path(path)
labels = ['AD',"CN","EMCI","MCI"]

data=[]
for disease_id , sp in enumerate(labels):
    for file in os.listdir(os.path.join(DATA_PATH, sp)):
        data.append(['{}/{}'.format(sp, file), disease_id, sp])
csv_data = pd.DataFrame(data, columns=['File', 'Disease Id', 'Disease Type'])
csv_data

import pandas as pd
import re

# CSV dosyasını okuyun
df = csv_data

def extract_subject_id(filename):
    # Adjust regex to match the specific pattern
    match = re.search(r'ADNI_(\d+_S_\d+)', filename)
    if match:
        return match.group(1)
    return None
# `file` sütunundan `subject_id`'yi çıkarıp yeni bir sütun oluşturun
df['subject_id'] = df['File'].apply(extract_subject_id)
# Yeni DataFrame'i yazdırın veya kaydedin
df

from sklearn.model_selection import train_test_split
# Get unique subject IDs
unique_subject_ids = df['subject_id'].unique()

train_val_subjects, test_subjects = train_test_split(
    unique_subject_ids,
    test_size=0.2,
    random_state=42
)

train_subjects, val_subjects = train_test_split(
    train_val_subjects,
    test_size=0.2,
    random_state=42
)
# Create train and test sets
train_df = df[df['subject_id'].isin(train_subjects)]
val_df = df[df['subject_id'].isin(val_subjects)]
test_df = df[df['subject_id'].isin(test_subjects)]

print("Train DataFrame:")
print(len(train_df))
print("\nVal DataFrame:")
print(len(val_df))
print("\nTest DataFrame:")
print(len(test_df))

X_train_item = np.zeros((train_df.shape[0], IMAGE_SIZE, IMAGE_SIZE,IMAGE_SIZE))
for i, file in tqdm(enumerate(train_df['File'].values)):
    loaded_data = np.load(path+"/"+file)
    if (loaded_data is not None):
        data_array = loaded_data[loaded_data.files[0]]
        X_train_item[i] = rescale_to_shape(data_array, target_shape)

X_train = (X_train_item - np.min(X_train_item)) / (np.max(X_train_item) - np.min(X_train_item))#normalize data
print('Train Shape: {}'.format(X_train.shape))

Y_train = train_df['Disease Id'].values
Y_train = to_categorical(Y_train, num_classes=4)

# Benzersiz array'leri bul
unique_arrays = np.unique(Y_train, axis=0)

# Benzersiz array'lerin sayısını yazdır
print("Benzersiz array'lerin sayısı:", unique_arrays.shape[0])

# Benzersiz array'leri yazdır
print("Benzersiz array'ler:")
print(unique_arrays)

X_val_item = np.zeros((val_df.shape[0], IMAGE_SIZE, IMAGE_SIZE,IMAGE_SIZE))
for i, file in tqdm(enumerate(val_df['File'].values)):
    loaded_data = np.load(path+"/"+file)
    if (loaded_data is not None):
        data_array = loaded_data[loaded_data.files[0]]
        X_val_item[i] = rescale_to_shape(data_array, target_shape)

X_val = (X_val_item - np.min(X_val_item)) / (np.max(X_val_item) - np.min(X_val_item))#normalize data
print('Validation Shape: {}'.format(X_val.shape))

Y_val = val_df['Disease Id'].values
Y_val = to_categorical(Y_val)

X_test_item = np.zeros((test_df.shape[0], IMAGE_SIZE, IMAGE_SIZE,IMAGE_SIZE))
for i, file in tqdm(enumerate(test_df['File'].values)):
    loaded_data = np.load(path+"/"+file)
    if (loaded_data is not None):
        data_array = loaded_data[loaded_data.files[0]]
        X_test_item[i] = rescale_to_shape(data_array, target_shape)

X_test = (X_test_item - np.min(X_test_item)) / (np.max(X_test_item) - np.min(X_test_item))#normalize data
print('Test Shape: {}'.format(X_test.shape))

Y_test = test_df['Disease Id'].values
Y_test = to_categorical(Y_test)

class_counts = csv_data['Disease Type'].value_counts()
plt.figure(figsize=(4, 4))
sns.barplot(x=class_counts.index, y=class_counts.values)
plt.xlabel('Labels')
plt.ylabel('Number of Samples')
plt.title('Label Distribution in the Dataset')
plt.xticks(rotation=45)
plt.show()

X_train = np.expand_dims(X_train, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)

X_test.shape

import tensorflow as tf
from tensorflow.keras import layers, models

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout

def create_3d_cnn_model(input_shape, num_classes):
    model = Sequential()

    # İlk 3D Convolutional Katman
    model.add(Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling3D(pool_size=(2, 2, 2)))

    # İkinci 3D Convolutional Katman
    model.add(Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu'))
    model.add(MaxPooling3D(pool_size=(2, 2, 2)))

    # Üçüncü 3D Convolutional Katman
    model.add(Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu'))
    model.add(MaxPooling3D(pool_size=(2, 2, 2)))

    # Dördüncü 3D Convolutional Katman
    model.add(Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu'))
    model.add(MaxPooling3D(pool_size=(2, 2, 2)))

    # Katıştırma
    model.add(Flatten())

    # Tam Bağlantılı Katman
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))

    # Çıkış Katmanı
    model.add(Dense(num_classes, activation='softmax'))

    return model

def residual_block(input_tensor, filters, kernel_size=(3, 3, 3), strides=(1, 1, 1)):
    x = layers.Conv3D(filters, kernel_size, strides=strides, padding='same')(input_tensor)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv3D(filters, kernel_size, padding='same')(x)
    x = layers.BatchNormalization()(x)

    # Skip connection
    shortcut = input_tensor
    if input_tensor.shape[-1] != filters:
        shortcut = layers.Conv3D(filters, kernel_size=(1, 1, 1), strides=strides, padding='same')(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)

    x = layers.add([x, shortcut])
    x = layers.Activation('relu')(x)

    return x

def build_3d_resnet(input_shape, num_classes):
    inputs = layers.Input(shape=input_shape)

    # Initial Conv3D layer
    x = layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), padding='same')(x)

    # Residual blocks
    x = residual_block(x, 64)
    x = residual_block(x, 64)

    x = residual_block(x, 128, strides=(2, 2, 2))
    x = residual_block(x, 128)

    x = residual_block(x, 256, strides=(2, 2, 2))
    x = residual_block(x, 256)

    x = residual_block(x, 512, strides=(2, 2, 2))
    x = residual_block(x, 512)

    # Global Average Pooling and output
    x = layers.Flatten()(x)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

input_shape = (IMAGE_SIZE,IMAGE_SIZE,IMAGE_SIZE,1)  # (height, width, depth, channels)
num_classes = 4

model = build_3d_resnet(input_shape=input_shape, num_classes=num_classes)

from keras.metrics import Precision, Recall,AUC,F1Score
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # lr=0.0001 , 0.1
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', F1Score()])

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True)

callbacks=[early_stopping]

X_train.shape

"""# 1.Eğitim"""

history = model.fit(X_train, Y_train,
                    validation_data=(X_val, Y_val),
                    epochs=64,  # Adjust the number of epochs based on your needs
                    batch_size=16,  # Adjust the batch size
                    verbose=1,
                    callbacks=callbacks)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \
    confusion_matrix

y_pred = model.predict([X_test])
y_pred = np.argmax(y_pred, axis=1).reshape(-1, 1)
Y_test = np.argmax(Y_test, axis=1).reshape(-1, 1)

print(" MODEL RESULTS")
print("Accuracy: ", accuracy_score(Y_test, y_pred))
print("F1_Score: ", f1_score(Y_test, y_pred, average='macro'))
print("Precision: ", precision_score(Y_test, y_pred, average='macro'))
print("Sensitivity: ", recall_score(Y_test, y_pred, average='macro'))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

cm = confusion_matrix(y_pred, Y_test)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=one_hot_encoder.categories_[0])
disp.plot(cmap=plt.cm.Blues)
plt.show()

