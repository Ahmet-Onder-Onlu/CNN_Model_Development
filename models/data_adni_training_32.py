# -*- coding: utf-8 -*-
"""data_adni_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mOBDkPF4TyAYi4BcV-8iY3ucqHL7IS9M
"""

import numpy as np
import scipy.ndimage
import matplotlib.pyplot as plt
import cv2
from pathlib import Path
import os
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, \
    confusion_matrix
from tensorflow.keras.utils import to_categorical
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, InputLayer, Conv2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization, Activation, ReLU, Flatten, Dense, Add,\
    Dropout,MaxPooling2D,Concatenate,Reshape
from tensorflow.keras.models import Sequential, Model
import tensorflow as tf
import warnings
from pathlib import Path
# Tüm uyarıları kapat
warnings.filterwarnings("ignore")
import seaborn as sns

IMAGE_SIZE = 224
target_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE)

def rescale_to_shape(array, target_shape):
    factors = [t / s for s, t in zip(array.shape, target_shape)]
    return scipy.ndimage.zoom(array, zoom=factors, mode='nearest')

path = "/home/grup17/data_npy"
DATA_PATH = Path(path)
labels = ['AD',"CN","EMCI","MCI"]

data=[]
for disease_id , sp in enumerate(labels):
    for file in os.listdir(os.path.join(DATA_PATH, sp)):
        data.append(['{}/{}'.format(sp, file), disease_id, sp])
csv_data = pd.DataFrame(data, columns=['File', 'Disease Id', 'Disease Type'])
csv_data

import pandas as pd
import re

# CSV dosyasını okuyun
df = csv_data

def extract_subject_id(filename):
    # Adjust regex to match the specific pattern
    match = re.search(r'ADNI_(\d+_S_\d+)', filename)
    if match:
        return match.group(1)
    return None
# `file` sütunundan `subject_id`'yi çıkarıp yeni bir sütun oluşturun
df['subject_id'] = df['File'].apply(extract_subject_id)
# Yeni DataFrame'i yazdırın veya kaydedin
df

from sklearn.model_selection import train_test_split
# Get unique subject IDs
unique_subject_ids = df['subject_id'].unique()

train_val_subjects, test_subjects = train_test_split(
    unique_subject_ids,
    test_size=0.2,
    random_state=42
)

train_subjects, val_subjects = train_test_split(
    train_val_subjects,
    test_size=0.2,
    random_state=42
)
# Create train and test sets
train_df = df[df['subject_id'].isin(train_subjects)]
val_df = df[df['subject_id'].isin(val_subjects)]
test_df = df[df['subject_id'].isin(test_subjects)]

print("Train DataFrame:")
print(len(train_df))
print("\nVal DataFrame:")
print(len(val_df))
print("\nTest DataFrame:")
print(len(test_df))

X_train_item = np.zeros((train_df.shape[0], IMAGE_SIZE, IMAGE_SIZE,IMAGE_SIZE))
for i, file in tqdm(enumerate(train_df['File'].values)):
    loaded_data = np.load(path+"/"+file)
    if (loaded_data is not None):
        data_array = loaded_data[loaded_data.files[0]]
        X_train_item[i] = rescale_to_shape(data_array, target_shape)

X_train = (X_train_item - np.min(X_train_item)) / (np.max(X_train_item) - np.min(X_train_item))#normalize data
print('Train Shape: {}'.format(X_train.shape))

Y_train = train_df['Disease Id'].values
Y_train = to_categorical(Y_train)

X_val_item = np.zeros((val_df.shape[0], IMAGE_SIZE, IMAGE_SIZE,IMAGE_SIZE))
for i, file in tqdm(enumerate(val_df['File'].values)):
    loaded_data = np.load(path+"/"+file)
    if (loaded_data is not None):
        data_array = loaded_data[loaded_data.files[0]]
        X_val_item[i] = rescale_to_shape(data_array, target_shape)

X_val = (X_val_item - np.min(X_val_item)) / (np.max(X_val_item) - np.min(X_val_item))#normalize data
print('Validation Shape: {}'.format(X_val.shape))

Y_val = val_df['Disease Id'].values
Y_val = to_categorical(Y_val)

X_test_item = np.zeros((test_df.shape[0], IMAGE_SIZE, IMAGE_SIZE,IMAGE_SIZE))
for i, file in tqdm(enumerate(test_df['File'].values)):
    loaded_data = np.load(path+"/"+file)
    if (loaded_data is not None):
        data_array = loaded_data[loaded_data.files[0]]
        X_test_item[i] = rescale_to_shape(data_array, target_shape)

X_test = (X_test_item - np.min(X_test_item)) / (np.max(X_test_item) - np.min(X_test_item))#normalize data
print('Test Shape: {}'.format(X_test.shape))

Y_test = test_df['Disease Id'].values
Y_test = to_categorical(Y_test)

class_counts = csv_data['Disease Type'].value_counts()
plt.figure(figsize=(4, 4))
sns.barplot(x=class_counts.index, y=class_counts.values)
plt.xlabel('Labels')
plt.ylabel('Number of Samples')
plt.title('Label Distribution in the Dataset')
plt.xticks(rotation=45)
plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, Dropout,Lambda
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model

# Giriş katmanı, gri görüntü olarak (224x224x1)
input_layer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, IMAGE_SIZE, 1))

# Gri görüntüleri 3 kanala dönüştürüyoruz (ResNet50'nin RGB bekleyen ilk katmanına uyacak şekilde)
def gray_to_rgb(x):
    return tf.image.grayscale_to_rgb(x)

# Lambda katmanı ile gri görüntüyü RGB'ye dönüştürüyoruz
rgb_images = TimeDistributed(Lambda(gray_to_rgb))(input_layer)

# ResNet50 modelini alıyoruz (önceden eğitilmiş, üst katmanlar olmadan)
cnn_base = ResNet50(weights='imagenet', include_top=False)

# ResNet50'yi TimeDistributed katmanına sarıyoruz
cnn_output = TimeDistributed(cnn_base)(rgb_images)

# Çıkışı düzleştiriyoruz
cnn_output_flat = TimeDistributed(Flatten())(cnn_output)

# LSTM katmanı ekliyoruz
lstm_output = LSTM(256, return_sequences=False)(cnn_output_flat)

# Son katman (sınıflandırma için)
output_layer = Dense(4, activation='softmax')(lstm_output)

# Modeli tanımlıyoruz
model = Model(inputs=input_layer, outputs=output_layer)

from keras.metrics import F1Score
# Modeli Derleme
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',F1Score()])

batch_size = 16
epochs = 150
hist_concat = model.fit(
    [X_train],Y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=([X_val],Y_val)
    )

y_pred = model.predict([X_test])
y_pred = np.argmax(y_pred, axis=1).reshape(-1, 1)
Y_test = np.argmax(Y_test, axis=1).reshape(-1, 1)

def model_result(Y_test,y_pred):
    print(" MODEL RESULTS")
    print("Accuracy: ", accuracy_score(Y_test, y_pred))
    print("F1_Score: ", f1_score(Y_test, y_pred, average='macro'))
    print("Precision: ", precision_score(Y_test, y_pred, average='macro'))
    print("Sensitivity: ", recall_score(Y_test, y_pred, average='macro'))

import matplotlib.pyplot as plt
def Plot_acc_history(hist_concat):
    plt.plot(hist_concat.history['accuracy'])
    plt.plot(hist_concat.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

def Plot_loss_history(hist_concat):
    plt.plot(hist_concat.history['loss'])
    plt.plot(hist_concat.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

import seaborn as sn
from sklearn.metrics import confusion_matrix
def Confusion_matrix(Y_test,y_pred):
    class_names = labels
    print('Test Confusion Matrix')
    cm_dense = confusion_matrix(Y_test, y_pred)
    sn.set(font_scale=1.2)  # for label size
    sn.heatmap(cm_dense, annot=True, fmt="d", linewidths=.5, annot_kws={"size": 16},xticklabels=class_names, yticklabels=class_names)

model_result(Y_test,y_pred)

Plot_acc_history(hist_concat)
Plot_loss_history(hist_concat)

Confusion_matrix(Y_test,y_pred)

